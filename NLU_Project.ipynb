{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boxed-annual",
   "metadata": {},
   "source": [
    "### For this this assignment we were asked to solve 5 points regarding the usage of the spaCy tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-selection",
   "metadata": {},
   "source": [
    "Read [spaCy documentation](https://spacy.io/api/dependencyparser) on dependency parser to learn provided methods.\n",
    "We start importing spaCy and loading the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-purse",
   "metadata": {},
   "source": [
    "I tested my functions with a common sentence often used during classes: ```\"I saw a man with a telescope.\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tamil-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "txt = \"I saw a man with a telescope.\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-elder",
   "metadata": {},
   "source": [
    "## 1Â° TASK: Extract a path of dependency relations from the ROOT to a token\n",
    "    - input is a sentence, you parse it and get a Doc object of spaCy\n",
    "    - for each token the path will be a list of dependency relations, where first element is ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-image",
   "metadata": {},
   "source": [
    "**For task 1, I definited the function** ```getDependecyPath(sentence)```\n",
    "\n",
    "INPUT: the sentence (type string) to parse\n",
    "\n",
    "OUTPUT: it returns a path as a list of dipendencies (from ROOT to token).\n",
    "\n",
    "I used ```token.sent.root``` and ```token.dep_``` respectively to find the sentence root and dependencies of each token (it gives the dependency relation between the token and its parent). \n",
    "\n",
    "These let me explore the arcs starting from the token up to the root in order to populate the list which in this way is sorted with the ROOT in the first place. So there was no need to reverse the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 1--------------------------------------------------\n",
    "\n",
    "def getDependecyPath(sentence):\n",
    "    \n",
    "    doc = nlp(sentence) #Construct of a Doc object. The most common way to get a Doc object is via the nlp object.\n",
    "    path = []\n",
    "\n",
    "    for token in doc:\n",
    "        tempList = []\n",
    "\n",
    "        while token != token.sent.root:\n",
    "            tempList.insert(0, token.text)\n",
    "            tempList.insert(0, ' ----('+token.dep_+')---> ')\n",
    "            token = token.sent.root\n",
    "\n",
    "        tempList.insert(0, token.text)\n",
    "        tempList.insert(0, ' ----('+token.dep_+')---> ')\n",
    "        tempList.insert(0, 'ROOT')\n",
    "        path.insert(0, tempList)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-least",
   "metadata": {},
   "source": [
    "Here the structure of the path as as shown below:\n",
    "\n",
    "```['ROOT', '----first dependency relation (ROOT)--->', 'Token', '----second dependency relation--->', 'Token']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "favorite-dispatch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(punct)---> ', '.']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(pobj)---> ', 'telescope']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(det)---> ', 'a']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(prep)---> ', 'with']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(dobj)---> ', 'man']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(det)---> ', 'a']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw']\n",
      "['ROOT', ' ----(ROOT)---> ', 'saw', ' ----(nsubj)---> ', 'I']\n"
     ]
    }
   ],
   "source": [
    "#Test Function 1\n",
    "\n",
    "dependencyPath = getDependecyPath(txt)\n",
    "\n",
    "for dp in dependencyPath:\n",
    "        print(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-edgar",
   "metadata": {},
   "source": [
    "displaCy is an named entity visualizer built-in in spaCy that I used as a dependency visualizer. displaCy is also able to detect that I'm working in a Jupyter notebook, this is why i set ```jupyter=True```. Here's a compact view of the dependency tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cultural-spencer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"974600eaa70047f2852047c802d9b0f9-0\" class=\"displacy\" width=\"1100\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">telescope.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,127.0 197.0,127.0 197.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-1\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M362,154.0 L358,146.0 366,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-2\" stroke-width=\"2px\" d=\"M212,152.0 212,102.0 500.0,102.0 500.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,154.0 L504.0,146.0 496.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-3\" stroke-width=\"2px\" d=\"M512,152.0 512,127.0 647.0,127.0 647.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M647.0,154.0 L651.0,146.0 643.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-4\" stroke-width=\"2px\" d=\"M812,152.0 812,127.0 947.0,127.0 947.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,154.0 L808,146.0 816,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-974600eaa70047f2852047c802d9b0f9-0-5\" stroke-width=\"2px\" d=\"M662,152.0 662,102.0 950.0,102.0 950.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-974600eaa70047f2852047c802d9b0f9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M950.0,154.0 L954.0,146.0 946.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(txt)\n",
    "displacy.render(doc, style=\"dep\",options={'compact':True},jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-mount",
   "metadata": {},
   "source": [
    "## 2Â° TASK: Extract subtree of a dependents given a token\n",
    "    - input is a sentence, you parse it and get a Doc object of spaCy\n",
    "    - for each token in Doc objects you extract a subtree of its dependents as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-haiti",
   "metadata": {},
   "source": [
    "**For task 2, I define the function** ```getSubtree(sentence)```\n",
    "\n",
    "INPUT: the sentence (type string) from which I extract the subtree\n",
    "\n",
    "OUTPUT: It return a subtree converted to list for each token in nlp(sentence)\n",
    "\n",
    "The attribute ```token.subtree``` is called for each token in the sentence, in order to obtain the subtree of dependents w.r.t the sentence order and it cast to list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "suburban-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 2--------------------------------------------------\n",
    "\n",
    "def getSubtree(sentence):\n",
    "\n",
    "    return {token.text: list(token.subtree) for token in nlp(sentence)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-message",
   "metadata": {},
   "source": [
    "To test the function I still gave the txt string as input. \n",
    "\n",
    "For each token, the subtree including the token itself is displayed as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "basic-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': [I],\n",
       " 'saw': [I, saw, a, man, with, a, telescope, .],\n",
       " 'a': [a],\n",
       " 'man': [a, man, with, a, telescope],\n",
       " 'with': [with, a, telescope],\n",
       " 'telescope': [a, telescope],\n",
       " '.': [.]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Function 2\n",
    "\n",
    "getSubtree(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-hardwood",
   "metadata": {},
   "source": [
    "## 3Â° TASK: check if a given list of tokens (segment of a sentence) forms a subtree\n",
    "    - you parse a sentence and get a Doc object of spaCy\n",
    "    - providing as an input ordered list of words, you output True/False based on the sequence forming a subtree or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-freedom",
   "metadata": {},
   "source": [
    "**For task 3, I used the function** ```is_Subtree(sentence, segment)```\n",
    "\n",
    "INPUT: the sentence (type string) and a segment as a list of tokens\n",
    "\n",
    "OUTPUT: It return TRUE if the sentence's tokens (I put in a subList every subtree extracted by ```token.subtrees```) match with all the segment's ones. FALSE otherwise. According to the definition of subtree infact \"A subtree of a tree T is a tree S consisting of a node in T and all of its descendants in T\". \n",
    "\n",
    "Only the main sentence and the test segment obtained as input is parsed using spaCy English model because the segmed is already a list of tokens. \n",
    "\n",
    "It is important to underline this condition ```len(segment) == 0:```, in fact an empty list forms a subtree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 3--------------------------------------------------\n",
    "\n",
    "def is_Subtree(sentence, segment):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    if len(segment) == 0:\n",
    "    \n",
    "        return True\n",
    "\n",
    "    for token in doc:\n",
    "        subList = token.subtree\n",
    "        if ([tok.text for tok in subList] == segment):\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-messenger",
   "metadata": {},
   "source": [
    "In this example test I gave as input 3 different list of token. \n",
    "\n",
    "**Bold** ```'\\033[1m' + txt + '\\033[0m'``` makes the print easily readble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "phantom-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of token ['a', 'telescope'] is a subtree of  \u001b[1mI saw a man with a telescope.\u001b[0m\n",
      "The list of token ['with', 'the', 'telescope'] is not a subtree of  \u001b[1mI saw a man with a telescope.\u001b[0m\n",
      "The list of token [] is a subtree of  \u001b[1mI saw a man with a telescope.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Test Function 3\n",
    "\n",
    "segment1 =['a','telescope']\n",
    "segment2 =['with','the','telescope']\n",
    "segment3 =[]\n",
    "\n",
    "    if is_Subtree(txt, segment1) == True:\n",
    "        print('The list of token', segment1, 'is a subtree of ', '\\033[1m' + txt + '\\033[0m')\n",
    "    else:\n",
    "        print('The list of token', segment1, 'is not a subtree of ', '\\033[1m' + txt + '\\033[0m')\n",
    "    \n",
    "    if is_Subtree(txt, segment2) == True:\n",
    "        print('The list of token', segment2, 'is a subtree of ', '\\033[1m' + txt + '\\033[0m')\n",
    "    else:\n",
    "        print('The list of token', segment2, 'is not a subtree of ', '\\033[1m' + txt + '\\033[0m')\n",
    "    \n",
    "    if is_Subtree(txt, segment3) == True: \n",
    "        print('The list of token', segment3, 'is a subtree of ', '\\033[1m' + txt + '\\033[0m')\n",
    "    else:\n",
    "        print('The list of token', segment3, 'is not a subtree of ', '\\033[1m' + txt + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-blogger",
   "metadata": {},
   "source": [
    "## 4Â° TASK: Identify head of a span, given its tokens\n",
    "    - input is a sequence of words (not necessarily a sentence)\n",
    "    - output is the head of the span (single word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-reader",
   "metadata": {},
   "source": [
    "**For task 4, I definited the function** ```getSpanHead(sentence)```\n",
    "\n",
    "INPUT: the sentence (type string). \n",
    "\n",
    "It's also possible to take in input a doc sentence that it's already parsed so it's necessary only to convert in span and extract the head/root with ```span.root```. If instead we already have a span in input, we can return directly.\n",
    "\n",
    "OUTPUT: It return a token corrisponding to the root of the span (a \"slice\" of a sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "stunning-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 4--------------------------------------------------\n",
    "\n",
    "def getSpanHead(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    span = doc[:]\n",
    "    \n",
    "    return span.root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-toner",
   "metadata": {},
   "source": [
    "I test my function with the usual string txt and with the method ```Doc.__getitem__``` that produces in the example a span consisting of tokens 1, 2, 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "tutorial-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The head of \" \u001b[1mA book of old english ballads\u001b[0m \" is book\n",
      "The head of \" book of old \"is book\n"
     ]
    }
   ],
   "source": [
    "#Test Function 4\n",
    "\n",
    "txt1 = 'A book of old english ballads'\n",
    "doc = nlp(txt1)\n",
    "span = doc[1:4]\n",
    "\n",
    "print ('The head of \\\"', '\\033[1m' + txt1 + '\\033[0m', '\\\" is' ,getSpanHead(txt1))\n",
    "print ('The head of \\\"',span,'\\\"is' ,getSpanHead(txt1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-alpha",
   "metadata": {},
   "source": [
    "## 5Â° TASK: Extract sentence subject, direct object and indirect object spans\n",
    "    - input is a sentence, you parse it and get a Doc object of spaCy\n",
    "    - output is lists of words that form a span for subject, direct object, and indirect obj (if present, otherwise empty)\n",
    "    - dict of lists, is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-residence",
   "metadata": {},
   "source": [
    "**For task 5, I definited the function** ```getSentenceDep(sentence)```\n",
    "\n",
    "INPUT: the sentence (type string) that is being parsed as always\n",
    "\n",
    "OUTPUT: It returns a dictionary with all identified lists of key dependency labels  \n",
    "\n",
    "The logic of this function si based on  subtree; It scans all the token in the sentences thanks to the subtree functionality that give us a sequence containing the token and all the tokenâs syntactic descendants. In this way it's possible to check the equality between the .dep_ and the predefined string. \n",
    "\n",
    "To categorize \"subjects\" I compared ```nsubj``` (nominal subject), ```nsubjpass``` (nominal subject of passive verbs), ```csubj``` (clausal subject, in case it's itself a clause) and ```csubjpass``` (clausal subject of passive verbs).\n",
    "\n",
    "For direct objects I only considered dobj and, in the end, the labels for indirect objects ```iobj``` and ```dative``` (towards whom or for an action is performed). \n",
    "**Spacy from version 3.1.0** replaced iobj with dative, so I decided to add both in case an older version is used.\n",
    "\n",
    "Once all tokens have been analysed, tokens are appended in the right category and the dictionary  is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "equipped-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function 5--------------------------------------------------\n",
    "\n",
    "def getSentenceDep(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    dict = {\n",
    "        'subjects':[], 'direct objects':[], 'indirect objects':[]\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "    \n",
    "        if(token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' \n",
    "           or token.dep_ == 'csubj' or token.dep_ == 'csubjpass'):\n",
    "            \n",
    "            for desc in token.subtree:\n",
    "                dict[\"subjects\"].append(desc.text)\n",
    "        \n",
    "        elif(token.dep_ == 'dobj'):\n",
    "            \n",
    "            for desc in token.subtree:\n",
    "                dict[\"direct objects\"].append(desc.text)\n",
    "   \n",
    "        elif(token.dep_ == 'dative' or token.dep_ == 'iobj'):\n",
    "        \n",
    "            for desc in token.subtree:\n",
    "                dict[\"indirect objects\"].append(desc.text)\n",
    "                \n",
    "    return dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "planned-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects, direct objects and indirect objects of the sentence \u001b[1mI saw a man with a telescope.\u001b[0m are: \n",
      " {'subjects': ['I'], 'direct objects': ['a', 'man', 'with', 'a', 'telescope'], 'indirect objects': []}\n"
     ]
    }
   ],
   "source": [
    "#Test Function 5\n",
    "\n",
    "print(\"Subjects, direct objects and indirect objects of the sentence\", '\\033[1m' + txt + '\\033[0m', \"are: \\n\", getSentenceDep(txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
